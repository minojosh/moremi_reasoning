{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bd2a77",
   "metadata": {},
   "source": [
    "# Salesforce BLIP3-OCR-200M Data Processing and Reasoning Trace Generation\n",
    "\n",
    "**Objective:** Utilize the `Salesforce/blip3-ocr-200m` dataset to extract OCR information and then generate reasoning traces using a specified language model. This notebook will be run in a Kaggle SSH environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23785d",
   "metadata": {},
   "source": [
    "## 1. Setup and Installations\n",
    "\n",
    "Install necessary Python packages for data handling, Hugging Face interactions, and any other dependencies required by the `multimodal_QRA_pair.py` script and the chosen models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b7f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure these packages are installed in your Kaggle environment\n",
    "# You might need to restart the kernel after running this cell for the first time.\n",
    "!pip install datasets pandas huggingface_hub pyarrow tqdm\n",
    "!git clone git@github.com:minojosh/moremi_reasoning.git \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc8101",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31767f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, snapshot_download\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Potentially more imports will be needed based on multimodal_QRA_pair.py\n",
    "# and the specific models used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93622af4",
   "metadata": {},
   "source": [
    "## 3. Configuration Loading\n",
    "\n",
    "Load project configurations. Critical paths like `data_path` and `image_dir` need to be correctly set for the Kaggle environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the configuration file (relative to this notebook)\n",
    "CONFIG_PATH = \"../config/reasoning_config.yaml\"\n",
    "PROMPTS_PATH = \"../config/reasoning_prompts.yaml\"\n",
    "\n",
    "def load_config(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "print(f\"Loading configuration from: {CONFIG_PATH}\")\n",
    "config = load_config(CONFIG_PATH)\n",
    "print(f\"Loading prompts from: {PROMPTS_PATH}\")\n",
    "prompts = load_config(PROMPTS_PATH)\n",
    "\n",
    "# --- CRITICAL: REVIEW AND UPDATE THESE PATHS FOR KAGGLE ---\n",
    "print(\"\\n--- CONFIGURATION REQUIRES YOUR ATTENTION ---\")\n",
    "print(f\"Current data_path: {config.get('data_path')}\")\n",
    "print(\"ACTION: Update 'data_path' in reasoning_config.yaml to the Kaggle path for Salesforce Parquet files.\")\n",
    "\n",
    "print(f\"Current image_dir: {config.get('image_dir')}\")\n",
    "print(\"ACTION: Update 'image_dir' in reasoning_config.yaml to the Kaggle path for images, if applicable.\")\n",
    "\n",
    "print(f\"Current model_name (for reasoning): {config.get('model_name')}\")\n",
    "print(f\"API URL (for reasoning model): {config.get('api_url')}\")\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Example of how you might set the Kaggle dataset path if you know it\n",
    "# KAGGLE_SALESFORCE_OCR_DATASET_PATH = \"/kaggle/input/salesforce-blip3-ocr-200m\"\n",
    "# config['data_path'] = os.path.join(KAGGLE_SALESFORCE_OCR_DATASET_PATH, \"parquet_files_subfolder_if_any\")\n",
    "# config['image_dir'] = os.path.join(KAGGLE_SALESFORCE_OCR_DATASET_PATH, \"image_files_subfolder_if_any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea9227",
   "metadata": {},
   "source": [
    "## 4. Load Salesforce BLIP3-OCR-200M Dataset\n",
    "\n",
    "Load the dataset. This might involve using `load_dataset` from Hugging Face `datasets` or directly reading Parquet files if downloaded via `huggingface_hub` or available through Kaggle datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using Hugging Face datasets library (if dataset is directly loadable and internet is available)\n",
    "# try:\n",
    "#     print(\"Attempting to load dataset via Hugging Face datasets library...\")\n",
    "#     ocr_dataset = load_dataset(\"Salesforce/blip3-ocr-200m\", split='train') # Or specific splits\n",
    "#     print(\"Dataset loaded successfully via load_dataset.\")\n",
    "#     print(ocr_dataset.info)\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not load dataset using load_dataset: {e}\")\n",
    "#     print(\"Falling back to manual download/load if paths are configured.\")\n",
    "#     ocr_dataset = None\n",
    "\n",
    "# Option 2: Manually specify path if downloaded or using Kaggle datasets\n",
    "# This assumes config['data_path'] has been correctly updated for your Kaggle environment.\n",
    "\n",
    "SALESFORCE_DATA_PATH = config.get('data_path', None)\n",
    "# IMPORTANT: Update config.get('data_path') in reasoning_config.yaml to your Kaggle path\n",
    "# For local testing, you might set it directly here, e.g.:\n",
    "# SALESFORCE_DATA_PATH = \"/path/to/your/downloaded/salesforce_ocr_data/001.parquet\"\n",
    "# OR if it's a directory: SALESFORCE_DATA_PATH = \"/path/to/your/downloaded/salesforce_ocr_data/\"\n",
    "\n",
    "raw_ocr_df = None\n",
    "\n",
    "if SALESFORCE_DATA_PATH and os.path.exists(SALESFORCE_DATA_PATH):\n",
    "    print(f\"Attempting to load data from: {SALESFORCE_DATA_PATH}\")\n",
    "    if os.path.isdir(SALESFORCE_DATA_PATH):\n",
    "        parquet_files = sorted([os.path.join(SALESFORCE_DATA_PATH, f) for f in os.listdir(SALESFORCE_DATA_PATH) if f.endswith('.parquet')])\n",
    "        if parquet_files:\n",
    "            print(f\"Found {len(parquet_files)} Parquet files. Loading the first one: {parquet_files[0]}\")\n",
    "            try:\n",
    "                raw_ocr_df = pd.read_parquet(parquet_files[0])\n",
    "                print(f\"Loaded dataframe from {parquet_files[0]}. Shape: {raw_ocr_df.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading Parquet file {parquet_files[0]}: {e}\")\n",
    "        else:\n",
    "            print(f\"No Parquet files found in directory: {SALESFORCE_DATA_PATH}.\")\n",
    "    elif SALESFORCE_DATA_PATH.endswith('.parquet'):\n",
    "        try:\n",
    "            raw_ocr_df = pd.read_parquet(SALESFORCE_DATA_PATH)\n",
    "            print(f\"Loaded dataframe from {SALESFORCE_DATA_PATH}. Shape: {raw_ocr_df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Parquet file {SALESFORCE_DATA_PATH}: {e}\")\n",
    "    else:\n",
    "        print(f\"Path '{SALESFORCE_DATA_PATH}' is not a recognized directory or .parquet file.\")\n",
    "elif SALESFORCE_DATA_PATH:\n",
    "    print(f\"Path '{SALESFORCE_DATA_PATH}' does not exist. Please verify.\")\n",
    "else:\n",
    "    print(\"'data_path' not configured in reasoning_config.yaml or is empty.\")\n",
    "\n",
    "if raw_ocr_df is not None:\n",
    "    print(\"\\nFirst 5 rows of the loaded data:\")\n",
    "    print(raw_ocr_df.head())\n",
    "    print(\"\\nColumns in the DataFrame:\", raw_ocr_df.columns.tolist())\n",
    "else:\n",
    "    print(\"\\nNo data loaded. Cannot proceed with exploration.\")\n",
    "    raw_ocr_df = pd.DataFrame() # Ensure it's a DataFrame to prevent errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44c82f",
   "metadata": {},
   "source": [
    "### 4.1. Specify Parquet File and Load Data\n",
    "\n",
    "Ensure `SALESFORCE_DATA_PATH` in Cell 7 (Configuration Loading) is correctly set to the directory containing your Parquet files in the Kaggle environment. We'll load one file to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming SALESFORCE_DATA_PATH is a directory containing the .parquet files\n",
    "# This was defined in Cell 7 based on your config.\n",
    "\n",
    "# Let's list available parquet files if the path is a directory\n",
    "parquet_file_to_load = None\n",
    "if SALESFORCE_DATA_PATH and os.path.exists(SALESFORCE_DATA_PATH) and os.path.isdir(SALESFORCE_DATA_PATH):\n",
    "    all_parquet_files = sorted([os.path.join(SALESFORCE_DATA_PATH, f) for f in os.listdir(SALESFORCE_DATA_PATH) if f.endswith('.parquet')])\n",
    "    if all_parquet_files:\n",
    "        parquet_file_to_load = all_parquet_files[0] # Load the first file\n",
    "        print(f\"Found {len(all_parquet_files)} Parquet files. Will load: {parquet_file_to_load}\")\n",
    "    else:\n",
    "        print(f\"ERROR: No .parquet files found in directory: {SALESFORCE_DATA_PATH}\")\n",
    "else:\n",
    "    print(f\"ERROR: SALESFORCE_DATA_PATH ('{SALESFORCE_DATA_PATH}') is not a valid directory or does not exist. Please check Cell 7.\")\n",
    "\n",
    "# Load the selected Parquet file into a Pandas DataFrame\n",
    "df = None\n",
    "if parquet_file_to_load:\n",
    "    try:\n",
    "        df = pd.read_parquet(parquet_file_to_load)\n",
    "        print(f\"Successfully loaded {parquet_file_to_load}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Parquet file {parquet_file_to_load}: {e}\")\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()\n",
    "    print(\"\\nDataFrame Head:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0973b42",
   "metadata": {},
   "source": [
    "### 4.2. Explore a Sample Row\n",
    "\n",
    "Let's examine the content of a single row, focusing on `uid`, `url`, `captions`, and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_sample_row(dataframe, index=0):\n",
    "    if dataframe is None or dataframe.empty:\n",
    "        print(\"DataFrame is not loaded or is empty.\")\n",
    "        return\n",
    "    if index >= len(dataframe):\n",
    "        print(f\"Index {index} is out of bounds for DataFrame of length {len(dataframe)}.\")\n",
    "        return\n",
    "\n",
    "    sample = dataframe.iloc[index]\n",
    "    print(f\"--- Exploring Sample Row (Index: {index}) ---\")\n",
    "    print(f\"UID: {sample.get('uid')}\")\n",
    "    print(f\"Key: {sample.get('key')}\")\n",
    "    image_url = sample.get('url')\n",
    "    print(f\"Image URL: {image_url}\")\n",
    "\n",
    "    print(\"\\n--- Parsed Captions (from 'captions' field) ---\")\n",
    "    captions_str = sample.get('captions')\n",
    "    if captions_str:\n",
    "        try:\n",
    "            captions_list = json.loads(captions_str)\n",
    "            for i, cap_obj in enumerate(captions_list):\n",
    "                print(f\"  Caption Entry {i}:\")\n",
    "                print(f\"    Granularity: {cap_obj.get('granularity')}\")\n",
    "                print(f\"    Include DataComp Raw Cap: {cap_obj.get('include_datacomp_raw_cap')}\")\n",
    "                # Limit printing of very long text fields\n",
    "                ocr_text = cap_obj.get('text', '')\n",
    "                print(f\"    Text: {ocr_text[:500] + ('...' if len(ocr_text) > 500 else '')}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  Error parsing 'captions' JSON: {e}\")\n",
    "            print(f\"  Raw captions string: {captions_str}\")\n",
    "    else:\n",
    "        print(\"  'captions' field is missing or empty.\")\n",
    "\n",
    "    print(\"\\n--- Parsed Metadata (from 'metadata' field) ---\")\n",
    "    metadata_str = sample.get('metadata')\n",
    "    if metadata_str:\n",
    "        try:\n",
    "            metadata_obj = json.loads(metadata_str)\n",
    "            print(f\"  Length (tokens): {metadata_obj.get('length')}\")\n",
    "            print(f\"  OCR UID: {metadata_obj.get('uid')}\")\n",
    "            entries = metadata_obj.get('entries', [])\n",
    "            print(f\"  Number of token entries: {len(entries)}\")\n",
    "            if entries:\n",
    "                print(\"    First 3 token entries (if available):\")\n",
    "                for i, entry in enumerate(entries[:3]):\n",
    "                    print(f\"      Token {i}: Text='{entry.get('text')}', Confidence={entry.get('confidence')}, BBox={entry.get('bbox')}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  Error parsing 'metadata' JSON: {e}\")\n",
    "            print(f\"  Raw metadata string: {metadata_str}\")\n",
    "    else:\n",
    "        print(\"  'metadata' field is missing or empty.\")\n",
    "\n",
    "if df is not None:\n",
    "    explore_sample_row(df, 0) # Explore the first row\n",
    "    if len(df) > 1:\n",
    "        explore_sample_row(df, 1) # Explore the second row if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2adcd6f",
   "metadata": {},
   "source": [
    "### 4.3. Formulate Simple Question & Answer from OCR Data\n",
    "\n",
    "Based on the exploration, we'll select an OCR text (e.g., from a specific granularity) to serve as the 'ground-truth answer' for a generic OCR question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocr_text_for_qna(sample_row, desired_granularity=0, prefer_no_raw_datacomp=True):\n",
    "    if sample_row is None:\n",
    "        return None\n",
    "    \n",
    "    captions_str = sample_row.get('captions')\n",
    "    if not captions_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        captions_list = json.loads(captions_str)\n",
    "        # Try to find the exact granularity, optionally filtering by include_datacomp_raw_cap\n",
    "        for cap_obj in captions_list:\n",
    "            if cap_obj.get('granularity') == desired_granularity:\n",
    "                if prefer_no_raw_datacomp and cap_obj.get('include_datacomp_raw_cap') == True:\n",
    "                    continue\n",
    "                return cap_obj.get('text')\n",
    "        \n",
    "        # Fallback: if exact granularity with preference not found, try without preference\n",
    "        if prefer_no_raw_datacomp:\n",
    "            for cap_obj in captions_list:\n",
    "                if cap_obj.get('granularity') == desired_granularity:\n",
    "                    return cap_obj.get('text')\n",
    "        \n",
    "        # Fallback: if desired_granularity not found at all, try to get any text from granularity 0 or 5\n",
    "        for cap_obj in captions_list:\n",
    "            if cap_obj.get('granularity') == 0:\n",
    "                 if prefer_no_raw_datacomp and cap_obj.get('include_datacomp_raw_cap') == True:\n",
    "                    continue\n",
    "                 return cap_obj.get('text')\n",
    "        for cap_obj in captions_list:\n",
    "            if cap_obj.get('granularity') == 5:\n",
    "                 if prefer_no_raw_datacomp and cap_obj.get('include_datacomp_raw_cap') == True:\n",
    "                    continue\n",
    "                 # Clean the XML-like tags from granularity 5 for a cleaner answer\n",
    "                 text_g5 = cap_obj.get('text')\n",
    "                 if text_g5:\n",
    "                     return re.sub(r'<ocr>([^<]+)</ocr><bbox>[^<]+</bbox>', r'\\1', text_g5).strip()\n",
    "                 return None # Should not happen if text_g5 exists\n",
    "                     \n",
    "        # Final fallback: return the first caption's text if any\n",
    "        if captions_list:\n",
    "            return captions_list[0].get('text')\n",
    "            \n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    sample_for_qna = df.iloc[0]\n",
    "    \n",
    "    # --- Configuration for Q&A Generation ---\n",
    "    # You can change this to experiment with different OCR outputs\n",
    "    # Granularity 0: Basic text extraction.\n",
    "    # Granularity 5: Text with OCR tags and bounding boxes (will be cleaned by get_ocr_text_for_qna).\n",
    "    CHOSEN_GRANULARITY_FOR_ANSWER = 0 \n",
    "    # --- End Configuration ---\n",
    "\n",
    "    ground_truth_ocr_text = get_ocr_text_for_qna(sample_for_qna, CHOSEN_GRANULARITY_FOR_ANSWER)\n",
    "    \n",
    "    if ground_truth_ocr_text:\n",
    "        question = \"What is all the text visible in this image?\"\n",
    "        image_uid = sample_for_qna.get('uid')\n",
    "        image_url_for_qna = sample_for_qna.get('url')\n",
    "        \n",
    "        print(f\"--- Generated Q&A for Image UID: {image_uid} ---\")\n",
    "        print(f\"Image URL: {image_url_for_qna}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Ground-Truth Answer (from Granularity {CHOSEN_GRANULARITY_FOR_ANSWER}, cleaned):\\n{ground_truth_ocr_text}\")\n",
    "        \n",
    "        # This is the data structure one item of your preprocessed list would look like for multimodal_QRA_pair.py\n",
    "        # (assuming multimodal_QRA_pair.py's main data loading is bypassed or adapted)\n",
    "        prepared_data_item = {\n",
    "            'process_id': image_uid, # Or some other unique ID\n",
    "            'Open-ended Verifiable Question': question,\n",
    "            'Ground-True Answer': ground_truth_ocr_text,\n",
    "            'img_urls': [image_url_for_qna] # multimodal_QRA_pair.py expects a list of URLs\n",
    "        }\n",
    "        print(\"\\nPrepared data item structure for the reasoning script:\")\n",
    "        print(json.dumps(prepared_data_item, indent=2))\n",
    "    else:\n",
    "        print(f\"Could not extract suitable OCR text for Q&A from sample row (UID: {sample_for_qna.get('uid')}).\")\n",
    "else:\n",
    "    print(\"DataFrame not loaded or empty, skipping Q&A formulation.\")\n",
    "\n",
    "# Make sure re is imported if not already\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09731850",
   "metadata": {},
   "source": [
    "## 5. OCR Data Processing and Reasoning Data Generation\n",
    "\n",
    "This section will use the `multimodal_QRA_pair.py` script's functionalities.\n",
    "\n",
    "**CRITICAL BLOCKER:** The `multimodal_QRA_pair.py` script is currently missing. Its logic needs to be integrated or callable from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for functionalities from multimodal_QRA_pair.py\n",
    "# Once multimodal_QRA_pair.py is available, its functions for processing OCR data\n",
    "# and generating question-reasoning-answer pairs will be called here.\n",
    "\n",
    "print(\"ACTION: Provide the 'multimodal_QRA_pair.py' script.\")\n",
    "print(\"This section cannot be implemented without it.\")\n",
    "\n",
    "# Example structure (highly speculative without the script):\n",
    "# def process_single_image_ocr(image_path, ocr_data_from_parquet_row):\n",
    "#     # ... logic from multimodal_QRA_pair.py ...\n",
    "#     pass\n",
    "\n",
    "# def generate_reasoning_trace(image_path, question, ocr_text):\n",
    "#     # ... logic using prompts from prompts.yaml and model from config.yaml ...\n",
    "#     pass\n",
    "\n",
    "# for index, row in tqdm(ocr_dataset.iterrows(), total=len(ocr_dataset)):\n",
    "#     # Assuming ocr_dataset is a pandas DataFrame loaded from Parquet\n",
    "#     image_id = row['uid'] # or 'key'\n",
    "#     image_url = row['url']\n",
    "#     captions_json = row['captions'] # This is a JSON string\n",
    "#     metadata_json = row['metadata'] # This is a JSON string\n",
    "    \n",
    "#     # Parse the JSON strings\n",
    "#     captions_data = json.loads(captions_json)\n",
    "#     metadata_data = json.loads(metadata_json)\n",
    "    \n",
    "#     # Determine image path (this needs robust handling based on how images are stored/accessed in Kaggle)\n",
    "#     # current_image_path = os.path.join(config.get('image_dir'), f\"{image_id}.jpg\") # Example, actual extension might vary\n",
    "    \n",
    "#     # Select desired OCR granularity from captions_data\n",
    "#     # For example, granularity 5 (text with OCR tags and bounding boxes)\n",
    "#     selected_ocr_text = None\n",
    "#     for cap in captions_data:\n",
    "#         if cap['granularity'] == 5 and not cap['include_datacomp_raw_cap']:\n",
    "#             selected_ocr_text = cap['text']\n",
    "#             break\n",
    "            \n",
    "#     if not selected_ocr_text:\n",
    "#         # Fallback or skip if desired granularity not found\n",
    "#         # print(f\"Granularity 5 not found for {image_id}\")\n",
    "#         continue\n",
    "        \n",
    "#     # TODO: Define a question for the image/OCR data\n",
    "#     # question = \"Describe the main subject and any prominent text in this image.\"\n",
    "    \n",
    "#     # if os.path.exists(current_image_path):\n",
    "#     #     reasoning_data = generate_reasoning_trace(current_image_path, question, selected_ocr_text)\n",
    "#     #     # Save or process reasoning_data\n",
    "#     # else:\n",
    "#     #     # print(f\"Image not found: {current_image_path}\")\n",
    "#     pass # End of loop placeholder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
